_base_:
  - cfgs/train/examples/lora_conventional.yaml

lora_unet:
  -
    lr: 1e-4
    rank: 8
    layers:
      - 're:.*\.attn.?$'
      - 're:.*\.ff$'

lora_text_encoder:
  - lr: 1e-5
    rank: 4
    # for both CLIP
    layers:
      - 're:.*self_attn$'
      - 're:.*mlp$'
    # for CLIP1
    # layers:
    #   - 're:clip_B.*self_attn$'
    #   - 're:clip_B.*mlp$'
    # for CLIP2
    # layers:
    #   - 're:clip_bigG.*self_attn$'
    #   - 're:clip_bigG.*mlp$'


model:
  pretrained_model_name_or_path: '/mnt/f/models/stable-diffusion-xl-base-1.0'
  clip_skip: 1
  clip_final_norm: False

data:
  dataset1:
    _target_: hcpdiff.data.CropInfoPairDataset
    batch_size: 4
    cache_latents: True

    source:
      data_source1:
        img_root: '/mnt/f/迅雷云盘/stable-diffusion-webui/APT/白玉_f/'
        prompt_template: 'prompt_tuning_template/object.txt'
        caption_file: '/mnt/f/迅雷云盘/stable-diffusion-webui/APT/白玉_f/image_captions.json' # path to image captions (file_words)

        word_names:
          pt1: baiyu

    bucket:
      _target_: hcpdiff.data.bucket.RatioBucket.from_files # aspect ratio bucket
      target_area: ${hcp.eval:512*768}
      num_bucket: 4

logger:
  - _target_: hcpdiff.loggers.CLILogger
    _partial_: True
    out_path: 'train.log'
    log_step: 20